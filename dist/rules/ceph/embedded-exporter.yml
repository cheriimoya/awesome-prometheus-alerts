groups:

- name: EmbeddedExporter

  rules:

    - alert: CephState
      expr: 'ceph_health_status != 0'
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph State"
        description: "Ceph instance unhealthy\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephMonitorClockSkew
      expr: 'abs(ceph_monitor_clock_skew_seconds) > 0.2'
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph monitor clock skew"
        description: "Ceph monitor clock skew detected. Please check ntp and hardware clock settings\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephMonitorLowSpace
      expr: 'ceph_monitor_avail_percent < 10'
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph monitor low space"
        description: "Ceph monitor storage is low.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephOsdDown
      expr: 'ceph_osd_up == 0'
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph OSD Down"
        description: "Ceph Object Storage Daemon Down\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephHighOsdLatency
      expr: 'ceph_osd_perf_apply_latency_seconds > 5'
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph high OSD latency"
        description: "Ceph Object Storage Daemon latency is high. Please check if it doesn't stuck in weird state.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephOsdLowSpace
      expr: 'ceph_osd_utilization > 90'
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph OSD low space"
        description: "Ceph Object Storage Daemon is going out of space. Please add more disks.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephOsdReweighted
      expr: 'ceph_osd_weight < 1'
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph OSD reweighted"
        description: "Ceph Object Storage Daemon takes too much time to resize.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephPgDown
      expr: 'ceph_pg_down > 0'
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph PG down"
        description: "Some Ceph placement groups are down. Please ensure that all the data are available.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephPgIncomplete
      expr: 'ceph_pg_incomplete > 0'
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph PG incomplete"
        description: "Some Ceph placement groups are incomplete. Please ensure that all the data are available.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephPgInconsistent
      expr: 'ceph_pg_inconsistent > 0'
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph PG inconsistent"
        description: "Some Ceph placement groups are inconsistent. Data is available but inconsistent across nodes.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephPgActivationLong
      expr: 'ceph_pg_activating > 0'
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph PG activation long"
        description: "Some Ceph placement groups are too long to activate.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephPgBackfillFull
      expr: 'ceph_pg_backfill_toofull > 0'
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph PG backfill full"
        description: "Some Ceph placement groups are located on full Object Storage Daemon on cluster. Those PGs can be unavailable shortly. Please check OSDs, change weight or reconfigure CRUSH rules.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

    - alert: CephPgUnavailable
      expr: 'ceph_pg_total - ceph_pg_active > 0'
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "{% if $labels.alias %}{{ $labels.alias }}{% else %}{{ $labels.instance }}: Ceph PG unavailable"
        description: "Some Ceph placement groups are unavailable.\nVALUE = {{ $value }}\n{{ range $k, $v := $labels }}{{ $k }}=\"{{ $v }}\" {{ end }}"

